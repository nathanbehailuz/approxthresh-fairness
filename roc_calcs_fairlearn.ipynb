{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "from fairlearn.postprocessing._constants import LABEL_KEY, SCORE_KEY\n",
    "from fairlearn.postprocessing._threshold_operation import ThresholdOperation\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from itertools import product\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "\n",
    "import warnings\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, accuracy_score, f1_score, roc_curve\n",
    "\n",
    "from fairlearn.metrics import equalized_odds_ratio\n",
    "\n",
    "from ACSIncomeDataLoader import  load_synthetic_data\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from typing import Callable\n",
    "\n",
    "import itertools\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extend_confusion_matrix(*, true_positives, false_positives, true_negatives, false_negatives):\n",
    "    return Bunch(\n",
    "        true_positives=true_positives,\n",
    "        false_positives=false_positives,\n",
    "        true_negatives=true_negatives,\n",
    "        false_negatives=false_negatives,\n",
    "        predicted_positives=(true_positives + false_positives),\n",
    "        predicted_negatives=(true_negatives + false_negatives),\n",
    "        positives=(true_positives + false_negatives),\n",
    "        negatives=(true_negatives + false_positives),\n",
    "        n=(true_positives + true_negatives + false_positives + false_negatives),\n",
    "    )\n",
    "\n",
    "\n",
    "METRIC_DICT = {\n",
    "    \"selection_rate\": lambda x: x.predicted_positives / x.n if x.n > 0 else 0,\n",
    "    \n",
    "    \"false_positive_rate\": lambda x: (\n",
    "        x.false_positives / x.negatives if x.negatives > 0 else 0\n",
    "    ),\n",
    "    \n",
    "    \"false_negative_rate\": lambda x: (\n",
    "        x.false_negatives / x.positives if x.positives > 0 else 0\n",
    "    ),\n",
    "    \n",
    "    \"true_positive_rate\": lambda x: (\n",
    "        x.true_positives / x.positives if x.positives > 0 else 0\n",
    "    ),\n",
    "    \n",
    "    \"true_negative_rate\": lambda x: (\n",
    "        x.true_negatives / x.negatives if x.negatives > 0 else 0\n",
    "    ),\n",
    "    \n",
    "    \"accuracy_score\": lambda x: (\n",
    "        (x.true_positives + x.true_negatives) / x.n if x.n > 0 else 0\n",
    "    ),\n",
    "    \n",
    "    \"balanced_accuracy_score\": lambda x: (\n",
    "        0.5 * (x.true_positives / x.positives if x.positives > 0 else 0) +\n",
    "        0.5 * (x.true_negatives / x.negatives if x.negatives > 0 else 0)\n",
    "    ),\n",
    "    \n",
    "    \"negative_predictive_value\": lambda x: (\n",
    "        x.true_negatives / (x.true_negatives + x.false_negatives) \n",
    "        if (x.true_negatives + x.false_negatives) > 0 else 0\n",
    "    ),\n",
    "    \n",
    "    \"precision\": lambda x: (\n",
    "        x.true_positives / (x.true_positives + x.false_positives)\n",
    "        if (x.true_positives + x.false_positives) > 0 else 0\n",
    "    ),\n",
    "    \n",
    "    \"recall\": lambda x: (\n",
    "        x.true_positives / (x.true_positives + x.false_negatives)\n",
    "        if (x.true_positives + x.false_negatives) > 0 else 0\n",
    "    ),\n",
    "    \n",
    "    \"f1_score\": lambda x: (\n",
    "        2 * (METRIC_DICT[\"precision\"](x) * METRIC_DICT[\"recall\"](x)) /\n",
    "        (METRIC_DICT[\"precision\"](x) + METRIC_DICT[\"recall\"](x))\n",
    "        if (METRIC_DICT[\"precision\"](x) + METRIC_DICT[\"recall\"](x)) > 0 else 0\n",
    "    )\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ROC(data: dict, calculate_ROC_points: Callable[[dict], list]):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for group_id, group_data in data.groupby('group'):\n",
    "        x, y, operation_list, objective_list, actual_counts_list, metrics_list = calculate_ROC_points(group_data)\n",
    "        plt.plot(x, y, marker='o', linestyle='-', label=f'Group {group_id}')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "    plt.xlabel('ADJ False Positive Rate')\n",
    "    plt.ylabel('ADJ True Positive Rate')\n",
    "    plt.title('ROC Curve by Group')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    return operation_list, objective_list, actual_counts_list, metrics_list\n",
    "\n",
    "\n",
    "def  plot_ROC_plotly(data: dict, calculate_ROC_points: Callable[[dict], list]):\n",
    "\n",
    "    fig = go.Figure()\n",
    "    for group_id, group_data in data.groupby('group'):\n",
    "        x, y, operation_list, objective_list, actual_counts_list, metrics_list = calculate_ROC_points(\n",
    "            group_data, \n",
    "            x_metric=\"false_positive_rate\", \n",
    "            y_metric=\"true_positive_rate\", \n",
    "            obj_metric=\"balanced_accuracy_score\"\n",
    "        )\n",
    "        fig.add_trace(go.Scatter(x=x, y=y, mode='lines+markers', name=f'Group {group_id}'))\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Random', line=dict(dash='dash')))\n",
    "    fig.update_layout(title='ROC Curve by Group', xaxis_title='ADJ False Positive Rate', yaxis_title='ADJ True Positive Rate')\n",
    "    fig.show()\n",
    "    return operation_list, objective_list, actual_counts_list, metrics_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interpolation_dict(thresholds_dict):\n",
    "    interpolation_dict = {}\n",
    "    for group, threshold in thresholds_dict.items():\n",
    "        op = ThresholdOperation(\">\", threshold)\n",
    "        # read from the repo \n",
    "        interpolation_dict[group] = Bunch(p0=1.0, operation0=op, p1=0.0, operation1=op)\n",
    "    return interpolation_dict\n",
    "\n",
    "\n",
    "def print_metrics(y_test, preds, group_test):\n",
    "    print('Balanced Accuracy: ', balanced_accuracy_score(y_test, preds))\n",
    "    print('ROC AUC: ', roc_auc_score(y_test, preds))\n",
    "    print('Accuracy: ', accuracy_score(y_test, preds))\n",
    "    print('F1 Score: ', f1_score(y_test, preds))\n",
    "    print('Equalized Odds Ratio: ', equalized_odds_ratio(y_test, preds, sensitive_features=group_test))\n",
    "    print()\n",
    "\n",
    "\n",
    "def _get_counts(labels):\n",
    "    \"\"\"Return the overall, positive, and negative counts of the labels.\n",
    "\n",
    "    :param labels: the labels of the samples\n",
    "    :type labels: list\n",
    "    :return: a tuple containing the overall, positive, and negative counts of the labels\n",
    "    :rtype: tuple of int, int, int\n",
    "    \"\"\"\n",
    "    n = len(labels)\n",
    "    n_positive = sum(labels)\n",
    "    n_negative = n - n_positive\n",
    "    return n, n_positive, n_negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ROC_points(data, \n",
    "                         x_metric=\"false_positive_rate\", \n",
    "                         y_metric=\"true_positive_rate\", \n",
    "                         obj_metric=\"balanced_accuracy_score\"):\n",
    "    scores = data['score'].tolist()\n",
    "    labels = data['label'].tolist()\n",
    "\n",
    "    data_sorted = data.sort_values(by=SCORE_KEY, ascending=False)\n",
    "\n",
    "    scores = list(data_sorted[SCORE_KEY])\n",
    "    labels = list(data_sorted[LABEL_KEY])\n",
    "\n",
    "    n, n_positive, n_negative = _get_counts(labels)\n",
    "    \n",
    "    scores.append(-np.inf)\n",
    "    labels.append(np.nan)\n",
    "    \n",
    "    thresholds = np.unique(scores)\n",
    "    thresholds = np.append(thresholds, thresholds[-1] + 1) \n",
    "    \n",
    "    i = 0\n",
    "    # count[0] -> false_positives \n",
    "    # count[1] -> true_positives \n",
    "    count = [0, 0]\n",
    "\n",
    "    x_list, y_list, operation_list, objective_list, actual_counts_list, metrics_list = [], [], [], [], [], []\n",
    "    while i < n:\n",
    "        # special handling of the initial point\n",
    "        if x_list == []:\n",
    "            threshold = np.inf\n",
    "        else:\n",
    "            threshold = scores[i]\n",
    "            \n",
    "            # calculate tp and fp for every threshold value\n",
    "            while scores[i] == threshold:\n",
    "                count[labels[i]] += 1\n",
    "                i += 1\n",
    "            threshold = (threshold + scores[i]) / 2\n",
    "\n",
    "        # calculate the rest of the metrics and store it\n",
    "        actual_counts = _extend_confusion_matrix(\n",
    "            false_positives=count[0],\n",
    "            true_positives=count[1],\n",
    "            true_negatives=(n_negative - count[0]),\n",
    "            false_negatives=(n_positive - count[1]),\n",
    "        )\n",
    "        actual_counts_list.append(actual_counts)\n",
    "\n",
    "        operations = [(\">\", actual_counts)]\n",
    "\n",
    "        '''\n",
    "        x = fpr , y = tpr , obj = balanced accuracy score\n",
    "        operation: a 'function' which compares given argument with threshold using operation_string\n",
    "        metrics: a dictionary of answers for all the metrics in METRIC_DICT\n",
    "        ''' \n",
    "        for operation_string, counts in operations:\n",
    "            if isinstance(x_metric, str) and isinstance(y_metric, str) and isinstance(obj_metric, str):\n",
    "                x = METRIC_DICT[x_metric](counts)\n",
    "                y = METRIC_DICT[y_metric](counts)\n",
    "                obj = METRIC_DICT[obj_metric](counts)\n",
    "                metrics = {metric: METRIC_DICT[metric](counts) for metric in METRIC_DICT}\n",
    "\n",
    "                operation = ThresholdOperation(operation_string, threshold)\n",
    "\n",
    "                x_list.append(x)\n",
    "                y_list.append(y)\n",
    "                operation_list.append(operation)\n",
    "                objective_list.append(obj)\n",
    "                metrics_list.append(metrics)\n",
    "            else:\n",
    "                raise ValueError(\"Metrics must be specified as strings corresponding to keys in METRIC_DICT.\")\n",
    "    \n",
    "    return x_list, y_list, operation_list, objective_list, actual_counts_list, metrics_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ApproxThresholdBrute():\n",
    "    def __init__(self, \n",
    "                y_score,\n",
    "                y_true,\n",
    "                group_assignments, \n",
    "                METRIC_DICT, \n",
    "                lambda_=0.5,\n",
    "                global_metric=\"f1_score\", \n",
    "                max_epsilon=1.0,\n",
    "                subsample=10000):\n",
    "        \n",
    "        self.METRIC_DICT = METRIC_DICT \n",
    "        self.y_score = y_score\n",
    "        self.y_true = y_true\n",
    "        self.group_assignments = group_assignments\n",
    "\n",
    "        self.lambda_ = lambda_\n",
    "        self.thresholds_ = None\n",
    "        self.epsilons_ = None\n",
    "        self.global_metric = global_metric\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.best_objective_value = None\n",
    "\n",
    "        self.data = pd.DataFrame({\n",
    "            'score': y_score,\n",
    "            'label': y_true,\n",
    "            'group': group_assignments\n",
    "        })\n",
    "\n",
    "        if len(self.data) > 10000 and subsample:\n",
    "            # raise warning brute force is slow\n",
    "            warnings.warn(\"Brute force is slow for large datasets.\"+\\\n",
    "                        \" Subsampling to 10000 samples. \\nYou \"+\\\n",
    "                        \"can disable this by setting subsample=False.\")\n",
    "            self.data = self.data.sample(10000)\n",
    "\n",
    "        unique_groups = self.data['group'].unique()\n",
    "\n",
    "        self.metrics_per_group_per_threshold = {}\n",
    "        all_threshold_lists = []\n",
    "    \n",
    "        for g in tqdm(unique_groups):\n",
    "            group_data = self.data[self.data['group'] == g]\n",
    "            \n",
    "            _, _, operation_list, _ , _ , metrics_list = calculate_ROC_points(group_data)\n",
    "\n",
    "            for all_metrics, operation in zip(metrics_list, operation_list):\n",
    "                if g not in self.metrics_per_group_per_threshold:\n",
    "                    self.metrics_per_group_per_threshold[g] = {}\n",
    "\n",
    "                # Debugger \n",
    "                if not all_metrics:  \n",
    "                    raise ValueError(f\"Invalid metrics computed for {g} with thresh {operation.threshold}\")\n",
    "                \n",
    "                self.metrics_per_group_per_threshold[g][operation.threshold] = all_metrics\n",
    "                                \n",
    "            \n",
    "            all_threshold_lists.append(operation_list)\n",
    "        \n",
    "        \n",
    "        possible_thresholds_combos = self.product_approach(all_threshold_lists)\n",
    " \n",
    "        \n",
    "        global_metrics_per_thresh_combo = {}\n",
    "        metric_vectors_per_thresh_combo = {}\n",
    "        group_size_vector = [len(self.data[self.data['group'] == g]) for g in unique_groups]\n",
    "        \n",
    "        for threshold_combo in possible_thresholds_combos:   \n",
    "            metrics_per_group = {}\n",
    "            global_metric_per_group = {}\n",
    "\n",
    "            for i, threshold in enumerate(threshold_combo):\n",
    "                '''\n",
    "                for given threshold and its index, find the respective metric values from metrics_per_group_per_threshold\n",
    "                and assign it to global_metric_per_group\n",
    "\n",
    "                assign global_metric_per_group to global_metrics_per_thresh_combo[tuple(threshold_combo)] \n",
    "                '''\n",
    "\n",
    "                thresh = float(threshold.threshold)\n",
    "                group = unique_groups[i]\n",
    "\n",
    "                if group not in self.metrics_per_group_per_threshold or thresh not in self.metrics_per_group_per_threshold[group]:\n",
    "                    raise ValueError(f\"Threshold {thresh} not found for group {group}\")\n",
    "        \n",
    "                metrics_per_group[group] = self.metrics_per_group_per_threshold[group][thresh]\n",
    "                global_metric_per_group[group] = self.metrics_per_group_per_threshold[group][thresh][self.global_metric]\n",
    "\n",
    "            global_metrics_per_thresh_combo[tuple(threshold_combo)] = global_metric_per_group\n",
    "            metric_vectors_per_thresh_combo[tuple(threshold_combo)] = metrics_per_group\n",
    "\n",
    "       \n",
    "        def get_metric_vectors_for_each_group(threshold_combo, metric_vectors_per_thresh_combo):\n",
    "            metric_vectors_for_each_group = []\n",
    "\n",
    "            for metric_dict in metric_vectors_per_thresh_combo[tuple(threshold_combo)].values():\n",
    "    \n",
    "                vector = np.array([float(val) for val in metric_dict.values()])\n",
    "                metric_vectors_for_each_group.append(vector)\n",
    "\n",
    "            return metric_vectors_for_each_group\n",
    "\n",
    "        \n",
    "        all_objective_values = []\n",
    "        for threshold_combo in possible_thresholds_combos:\n",
    "            objective_value, thresholds_used, epsilon_diffs = self.objective_func(\n",
    "                global_metrics_per_thresh_combo[tuple(threshold_combo)],\n",
    "                group_size_vector,\n",
    "                get_metric_vectors_for_each_group(threshold_combo,metric_vectors_per_thresh_combo),\n",
    "                threshold_combo,\n",
    "                lambda_val=self.lambda_,\n",
    "                diff_strategy='euclidean'\n",
    "            )\n",
    "            all_objective_values.append((objective_value, thresholds_used, epsilon_diffs))\n",
    "\n",
    "\n",
    "        sorted_objective_values = sorted(all_objective_values, key=lambda x: x[0])\n",
    "        \n",
    "        best_objective_value, best_threshold_combo, best_epsilon_diff = sorted_objective_values[0]\n",
    "\n",
    "        self.best_objective_value = best_objective_value\n",
    "        self.best_threshold_combo = best_threshold_combo\n",
    "        self.best_epsilon_diff = best_epsilon_diff\n",
    "\n",
    "        print(\"Best Objective Value:\", best_objective_value)\n",
    "        print(\"Best Threshold Combination:\", best_threshold_combo)\n",
    "        # print(\"Best Epsilon Difference:\", best_epsilon_diff)\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_func(global_metric_for_each_group,\n",
    "                    group_size_vector,\n",
    "                    metric_vectors_for_each_group, \n",
    "                    threshold_combo,\n",
    "                    lambda_val=0.5,\n",
    "                    diff_strategy='euclidean'):\n",
    "\n",
    "        group_metrics = np.array([global_metric_for_each_group[group] for group in global_metric_for_each_group.keys()])\n",
    "        \n",
    "        \n",
    "        global_performance_weighted = np.average(group_metrics, weights=group_size_vector)\n",
    "\n",
    "        objective = 0\n",
    "        epsilon_diffs = {}\n",
    "\n",
    "        # Check and adjust dimensions of vectors\n",
    "        for i, group_vector in enumerate(metric_vectors_for_each_group):\n",
    "            for j, other_group_vector in enumerate(metric_vectors_for_each_group):\n",
    "                if i != j:\n",
    "                    # Shapes match?\n",
    "                    if group_vector.shape != other_group_vector.shape:\n",
    "                        raise ValueError(f\"Dimension mismatch: Group {i} ({group_vector.shape}) vs Group {j} ({other_group_vector.shape})\")\n",
    "\n",
    "                    if diff_strategy == 'euclidean':\n",
    "                        distances = cdist(\n",
    "                            np.array(group_vector).reshape(1, -1), \n",
    "                            np.array(other_group_vector).reshape(1, -1), \n",
    "                            metric='euclidean'\n",
    "                        )\n",
    "                    elif diff_strategy == 'cosine':\n",
    "                        distances = np.dot(group_vector, other_group_vector) / (\n",
    "                            np.linalg.norm(group_vector) * np.linalg.norm(other_group_vector))\n",
    "                    else:\n",
    "                        raise ValueError(f\"Invalid diff strategy: {diff_strategy}\")\n",
    "                    \n",
    "                    objective += np.sum(distances)\n",
    "                    epsilon_diff = np.abs(group_vector - other_group_vector)\n",
    "                    epsilon_diffs[(i, j)] = epsilon_diff\n",
    "\n",
    "        \n",
    "        n_unique_groups = len(metric_vectors_for_each_group)\n",
    "        A_choose_2 = n_unique_groups * (n_unique_groups - 1) / 2\n",
    "\n",
    "        \n",
    "        objective = (1 - lambda_val) * (objective / A_choose_2) + lambda_val * (1 - global_performance_weighted)\n",
    "        return objective, threshold_combo, epsilon_diffs\n",
    "\n",
    "    @staticmethod\n",
    "    def product_approach(all_lists, use_numpy=True):\n",
    "        def numpy_cartesian_product(lists):\n",
    "            arrays = [np.array(lst) for lst in lists]\n",
    "            grid = np.meshgrid(*arrays, indexing='ij')\n",
    "            return np.stack(grid, axis=-1).reshape(-1, len(arrays))\n",
    "\n",
    "        if use_numpy:\n",
    "            return numpy_cartesian_product(all_lists)\n",
    "        else:\n",
    "            return list(product(*all_lists))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_counts(labels):\n",
    "        \"\"\"Return the overall, positive, and negative counts of the labels.\n",
    "\n",
    "        :param labels: the labels of the samples\n",
    "        :type labels: list\n",
    "        :return: a tuple containing the overall, positive, and negative counts of the labels\n",
    "        :rtype: tuple of int, int, int\n",
    "        \"\"\"\n",
    "        n = len(labels)\n",
    "        n_positive = sum(labels)\n",
    "        n_negative = n - n_positive\n",
    "        return n, n_positive, n_negative\n",
    "\n",
    "    @staticmethod\n",
    "    def _extend_confusion_matrix( \n",
    "            *, true_positives, false_positives, true_negatives, false_negatives\n",
    "        ):\n",
    "            return Bunch(\n",
    "                true_positives=true_positives,\n",
    "                false_positives=false_positives,\n",
    "                true_negatives=true_negatives,\n",
    "                false_negatives=false_negatives,\n",
    "                predicted_positives=(true_positives + false_positives),\n",
    "                predicted_negatives=(true_negatives + false_negatives),\n",
    "                positives=(true_positives + false_negatives),\n",
    "                negatives=(true_negatives + false_positives),\n",
    "                n=(true_positives + true_negatives + false_positives + false_negatives),\n",
    "            )\n",
    "\n",
    "    # haven't used this function \n",
    "    @staticmethod\n",
    "    def calculate_ROC_points(data, METRIC_DICT):\n",
    "        data = data.copy()\n",
    "        data_sorted = data.sort_values(by=SCORE_KEY, ascending=False)\n",
    "\n",
    "        scores = list(data_sorted['score'])\n",
    "        labels = list(data_sorted['label'])\n",
    "\n",
    "        n, n_positive, n_negative = ApproxThresholdBrute._get_counts(labels)\n",
    "        \n",
    "        # Use actual score values for thresholds\n",
    "        thresholds = np.unique(scores)\n",
    "        operation_list, actual_counts_list, metrics_list = [], [], []\n",
    "\n",
    "        for threshold in thresholds:\n",
    "            predictions = (scores > threshold).astype(int)\n",
    "            tp = np.sum((predictions == 1) & (labels == 1))\n",
    "            fp = np.sum((predictions == 1) & (labels == 0))\n",
    "            tn = np.sum((predictions == 0) & (labels == 0))\n",
    "            fn = np.sum((predictions == 0) & (labels == 1))\n",
    "\n",
    "            actual_counts = ApproxThresholdBrute._extend_confusion_matrix(\n",
    "                false_positives=fp,\n",
    "                true_positives=tp,\n",
    "                true_negatives=tn,\n",
    "                false_negatives=fn,\n",
    "            )\n",
    "            \n",
    "            metrics = {metric: METRIC_DICT[metric](actual_counts) for metric in METRIC_DICT}\n",
    "            operation = ThresholdOperation(\">\", float(threshold))\n",
    "            \n",
    "            operation_list.append(operation)\n",
    "            actual_counts_list.append(actual_counts)\n",
    "            metrics_list.append(metrics)\n",
    "\n",
    "        return operation_list, actual_counts_list, metrics_list\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_batch_combinations(possible_thresholds, start, end):\n",
    "        \"\"\"Generate combinations for a batch of thresholds\"\"\"\n",
    "        first_group_thresholds = possible_thresholds[0][start:end]\n",
    "        other_groups_thresholds = possible_thresholds[1:]\n",
    "        \n",
    "        for combo in itertools.product(first_group_thresholds, *other_groups_thresholds):\n",
    "            yield combo\n",
    "\n",
    "    def f1_score(self, metric):\n",
    "        \"\"\"\n",
    "        metric: a dictionary of metrics for each group\n",
    "\n",
    "        \"\"\"\n",
    "        # Extract the dictionary from the NumPy array\n",
    "        return (metric.item()['f1_score'])\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test():\n",
    "    X = pd.DataFrame([[1], [1], [0], [0], [1], [1], [0], [0]], columns=[\"feature\"])\n",
    "    Y = pd.DataFrame([0, 0, 0, 0, 1, 1, 1, 1], columns=[\"label\"])  \n",
    "    A = pd.DataFrame([1, 1, 0, 0, 0, 0, 1, 1], columns=[\"group\"]) \n",
    "\n",
    "    y = Y.values.ravel()\n",
    "    X_train, X_test, y_train, y_test, A_train, A_test = train_test_split(\n",
    "        X, y, A, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    dummy_clf = DummyClassifier(strategy=\"stratified\", random_state=42)\n",
    "    dummy_clf.fit(X_train, y_train)\n",
    "\n",
    "    preds_proba = dummy_clf.predict_proba(X_train)[:, 1]\n",
    "    y_pred = dummy_clf.predict(X_test)\n",
    "\n",
    "    def compute_fpr_tpr_objective(y_true, y_pred, sensitive_features):\n",
    "        unique_groups = np.unique(sensitive_features)\n",
    "        tpr_per_group = {}\n",
    "        fpr_per_group = {}\n",
    "\n",
    "        # Compute TPR and FPR for each group\n",
    "        for group in unique_groups:\n",
    "            indices = (sensitive_features == group)\n",
    "            y_true_group = y_true[indices]\n",
    "            y_pred_group = y_pred[indices]\n",
    "\n",
    "            tn, fp, fn, tp = confusion_matrix(y_true_group, y_pred_group).ravel()\n",
    "            tpr_per_group[group] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            fpr_per_group[group] = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "\n",
    "        \n",
    "        tpr_diff = max(tpr_per_group.values()) - min(tpr_per_group.values())\n",
    "        fpr_diff = max(fpr_per_group.values()) - min(fpr_per_group.values())\n",
    "        objective_value = max(tpr_diff, fpr_diff)\n",
    "\n",
    "        return tpr_per_group, fpr_per_group, objective_value\n",
    "\n",
    "    tpr_per_group, fpr_per_group, dummy_objective_value = compute_fpr_tpr_objective(\n",
    "        y_train, dummy_clf.predict(X_train), A_train.values.ravel()\n",
    "    )\n",
    "\n",
    "    print(\"Dummy Classifier Results:\")\n",
    "    print(f\"TPR per group: {tpr_per_group}\")\n",
    "    print(f\"FPR per group: {fpr_per_group}\")\n",
    "    print(f\"Objective Value: {dummy_objective_value}\")\n",
    "\n",
    "\n",
    "    data = pd.DataFrame({\n",
    "        \"score\": preds_proba,    \n",
    "        \"label\": y_train,        \n",
    "        \"group\": A_train.values.ravel()  \n",
    "    })\n",
    "\n",
    "    at = ApproxThresholdBrute(\n",
    "        y_score=preds_proba,\n",
    "        y_true=y_train,\n",
    "        group_assignments=A_train.values.ravel(),\n",
    "        METRIC_DICT=METRIC_DICT,\n",
    "        lambda_=0.5\n",
    "    )\n",
    "\n",
    "    best_objective_value = at.best_objective_value\n",
    "    print(\"ApproxThresholdBrute Results:\")\n",
    "    print(f\"Best Objective Value: {best_objective_value}\")\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dummy Classifier Results:\n",
      "TPR per group: {0: 0.0, 1: 0.5}\n",
      "FPR per group: {0: 0.5, 1: 1.0}\n",
      "Objective Value: 0.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c465deb0744c66b3b2900ecb9d746c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Objective Value: 0.7337684871413404\n",
      "Best Threshold Combination: [[>-inf] [>-inf]]\n",
      "ApproxThresholdBrute Results:\n",
      "Best Objective Value: 0.7337684871413404\n"
     ]
    }
   ],
   "source": [
    "run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db3f5888082d47bba1032f9b2844ccec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Objective Value: 0.09595959595959597\n",
      "Best Threshold Combination: [[>0.46199627220630646] [>0.5055713281035423]]\n",
      "Balanced Accuracy:  0.7708333333333333\n",
      "ROC AUC:  0.7708333333333333\n",
      "Accuracy:  0.75\n",
      "F1 Score:  0.761904761904762\n",
      "Equalized Odds Ratio:  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, Y, A = load_synthetic_data()\n",
    "\n",
    "random_indices = np.random.choice(X.index, size=100, replace=False)\n",
    "X = X.loc[random_indices] \n",
    "Y = Y.loc[random_indices] \n",
    "A = A.loc[random_indices]\n",
    "\n",
    "y = Y.values\n",
    "\n",
    "X_train, X_test, y_train, y_test, A_train, A_test = train_test_split( X, y, A, test_size=0.2, random_state=42)\n",
    "model = xgb.XGBClassifier()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "preds_proba = model.predict_proba(X_train)[:, 1]\n",
    "\n",
    "data = pd.DataFrame({\n",
    "        'score': preds_proba,\n",
    "        'label': y_train,\n",
    "        'group': A_train.values \n",
    "        })\n",
    "\n",
    "at = ApproxThresholdBrute(y_score=preds_proba, y_true=y_train, group_assignments=A_train.values, METRIC_DICT=METRIC_DICT)\n",
    "\n",
    "# operation_list, objective_list, actual_counts_list, metrics_list = plot_ROC_plotly(data, calculate_ROC_points)\n",
    "\n",
    "print_metrics(y_test, model.predict(X_test), A_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
